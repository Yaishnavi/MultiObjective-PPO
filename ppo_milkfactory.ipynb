{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('rl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "accc2f43be2e1f6f04b542269f945672579f4d887acf3251fbd55070909a0f05"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym, os\n",
    "from itertools import count\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(Model, self).__init__()\n",
    "        self.affine = nn.Linear(state_dim, n_latent_var)\n",
    "        \n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim = -1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "        # Memory:\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, state, action=None, evaluate=False):\n",
    "        # if evaluate is True then we also need to pass an action for evaluation\n",
    "        # else we return a new action from distribution\n",
    "        if not evaluate:\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        action_probs = self.action_layer(state)\n",
    "        action_distribution = Categorical(action_probs)\n",
    "        \n",
    "        if not evaluate:\n",
    "            action = action_distribution.sample()\n",
    "            self.actions.append(action)\n",
    "            \n",
    "        self.logprobs.append(action_distribution.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        if evaluate:\n",
    "            return action_distribution.entropy().mean()\n",
    "        \n",
    "        if not evaluate:\n",
    "            return action.item()\n",
    "        \n",
    "    def clearMemory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = Model(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(),\n",
    "                                              lr=lr, betas=betas)\n",
    "        self.policy_old = Model(state_dim, action_dim, n_latent_var).to(device)\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.kl = 0\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(self.policy_old.rewards):\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list in tensor\n",
    "        old_states = torch.tensor(self.policy_old.states).to(device).detach()\n",
    "        old_actions = torch.tensor(self.policy_old.actions).to(device).detach()\n",
    "        old_logprobs = torch.tensor(self.policy_old.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            dist_entropy = self.policy(old_states, old_actions, evaluate=True)\n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            logprobs = self.policy.logprobs[0].to(device)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            self.kl =F.kl_div(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            state_values = self.policy.state_values[0].to(device)\n",
    "            advantages = rewards - state_values.squeeze().detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.policy.clearMemory()\n",
    "            \n",
    "        self.policy_old.clearMemory()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pygame 1.9.6\nHello from the pygame community. https://www.pygame.org/contribute.html\n#################################################\nGame environment MILK FACTORY is created !!!\nSeed: 5428\n#################################################\n1\n3\n"
     ]
    }
   ],
   "source": [
    "import fruit.envs.games.milk_factory.engine as mf\n",
    "from fruit.envs.juice import FruitEnvironment\n",
    "\n",
    "game = mf.MilkFactory(render=False, speed=6000, max_frames=200, frame_skip=1, number_of_milk_robots=2, number_of_fix_robots=1, number_of_milks=2, seed=None, human_control=False, error_freq=0.01, human_control_robot=0, milk_speed=3, debug=False, action_combined_mode=False, show_status=False,number_of_exits=2)\n",
    "env = FruitEnvironment(game)\n",
    "print(env.get_number_of_objectives())\n",
    "print(env.get_number_of_agents()) # here number of agents is the number of objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 300*360*3\n",
    "action_dim = 5\n",
    "n_obj = env.get_number_of_objectives()\n",
    "n_episodes = 30\n",
    "max_timesteps = 500\n",
    "kl_param = 0.1\n",
    "log_interval = 10\n",
    "n_latent_var = 64           # number of variables in hidden layer\n",
    "lr = 0.0007\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = None\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "filename = \"PPO_MilkCollector.pth\"\n",
    "directory = \"./preTrained/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9c2c79ee2bce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Running policy_old:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mstate_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vaish/Desktop/PPO/fruit/envs/juice.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vaish/Desktop/PPO/fruit/envs/games/milk_factory/engine.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_control\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_of_objs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m                 \u001b[0mrobot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "running_reward = 0\n",
    "rewards = np.zeros((n_episodes,n_obj))\n",
    "for ep in range(1, n_episodes+1):\n",
    "    state = np.random.rand((300*360*3))\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.policy_old(state)\n",
    "        reward = env.step(action)\n",
    "        state_n = env.get_state()\n",
    "        done = env.is_terminal()\n",
    "        # Saving state and reward:\n",
    "        ppo.policy_old.states.append(state)\n",
    "        ppo.policy_old.rewards.append(reward)\n",
    "        state = np.array([state_n]).reshape(1,300*360*3)\n",
    "        running_reward +=reward \n",
    "        if done:\n",
    "            # save model\n",
    "            torch.save(ppo.policy.state_dict(), directory+filename+str(0))\n",
    "            rewards[ep-1,0]= running_reward\n",
    "            break\n",
    "        running_reward = 0\n",
    "\n",
    "    for k in range(1,n_obj):\n",
    "        state = np.random.rand((300*360*3))\n",
    "        for t in range(max_timesteps):\n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old(state)\n",
    "            reward = env.step(action)\n",
    "            state_n = env.get_state()\n",
    "            done = env.is_terminal()\n",
    "            kl = ppo.kl\n",
    "            new_reward = reward + (kl_param*kl)\n",
    "\n",
    "            # Saving state and reward:\n",
    "            ppo.policy_old.states.append(state)\n",
    "            ppo.policy_old.rewards.append(new_reward)\n",
    "            state = np.array([state_n]).reshape(1,300*360*3)\n",
    "            running_reward += new_reward \n",
    "            if done:\n",
    "                # save model\n",
    "                torch.save(ppo.policy.state_dict(), directory+filename+str(k))\n",
    "                rewards[ep-1,k]= running_reward\n",
    "                break\n",
    "\n",
    "    print('Episode: {}\\tReward: {}'.format(ep, rewards[ep-1,:]))\n",
    "    running_reward = 0\n",
    "\n",
    "plt.plot(np.arange(len(rewards[:,0])), rewards[:,0], label = 'reward1')\n",
    "plt.plot(np.arange(len(rewards[:,1])), rewards[:,1], label = 'reward2')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.savefig('milkfactory',bbox_inches='tight',facecolor=\"#FFFFFF\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "running_reward = 0\n",
    "rewards = []\n",
    "for ep in range(1, n_episodes+1):\n",
    "    state = np.random.rand((300*325*3))\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.policy_old(state)\n",
    "        reward = env.step(action)\n",
    "        state_n = env.get_state()\n",
    "        done = env.is_terminal()\n",
    "        # Saving state and reward:\n",
    "        ppo.policy_old.states.append(state)\n",
    "        ppo.policy_old.rewards.append(reward)\n",
    "        state = np.array([state_n]).reshape(1,300*360*3)\n",
    "        running_reward +=reward \n",
    "        if done:\n",
    "            # save model\n",
    "            torch.save(ppo.policy.state_dict(), directory+filename)\n",
    "            rewards.append(running_reward)\n",
    "            break\n",
    "\n",
    "    print('Episode: {}\\tReward: {}'.format(ep, int(running_reward)))\n",
    "    running_reward = 0\n",
    "\n",
    "plt.plot(np.arange(len(rewards)), rewards, label = 'reward1')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.savefig('foodcollector_base',bbox_inches='tight',facecolor=\"#FFFFFF\")\n",
    "plt.show()  \n"
   ]
  }
 ]
}