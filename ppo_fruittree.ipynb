{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('rl': conda)",
   "metadata": {
    "interpreter": {
     "hash": "accc2f43be2e1f6f04b542269f945672579f4d887acf3251fbd55070909a0f05"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import gym, os\n",
    "from itertools import count\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from math import log2\n",
    "import pdb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var):\n",
    "        super(Model, self).__init__()\n",
    "        self.affine = nn.Linear(state_dim, n_latent_var)\n",
    "        \n",
    "        # actor\n",
    "        self.action_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, action_dim),\n",
    "                nn.Softmax(dim = -1)\n",
    "                )\n",
    "        \n",
    "        # critic\n",
    "        self.value_layer = nn.Sequential(\n",
    "                nn.Linear(state_dim, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, n_latent_var),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(n_latent_var, 1)\n",
    "                )\n",
    "        \n",
    "        # Memory:\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.state_values = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def forward(self, state, action=None, evaluate=False):\n",
    "        # if evaluate is True then we also need to pass an action for evaluation\n",
    "        # else we return a new action from distribution\n",
    "        if not evaluate:\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        state_value = self.value_layer(state)\n",
    "        \n",
    "        action_probs = self.action_layer(state)\n",
    "        action_distribution = Categorical(action_probs)\n",
    "        \n",
    "        if not evaluate:\n",
    "            action = action_distribution.sample()\n",
    "            self.actions.append(action)\n",
    "            \n",
    "        self.logprobs.append(action_distribution.log_prob(action))\n",
    "        self.state_values.append(state_value)\n",
    "        \n",
    "        if evaluate:\n",
    "            return action_distribution.entropy().mean()\n",
    "        \n",
    "        if not evaluate:\n",
    "            return action.item()\n",
    "        \n",
    "    def clearMemory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.state_values[:]\n",
    "        del self.rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.policy = Model(state_dim, action_dim, n_latent_var).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(),\n",
    "                                              lr=lr, betas=betas)\n",
    "        self.policy_old = Model(state_dim, action_dim, n_latent_var).to(device)\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        self.kl = 0\n",
    "\n",
    "    def update(self):   \n",
    "        # Monte Carlo estimate of state rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(self.policy_old.rewards):\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "        \n",
    "        # Normalizing the rewards:\n",
    "        rewards = torch.tensor(rewards).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "        \n",
    "        # convert list in tensor\n",
    "        old_states = torch.tensor(self.policy_old.states).to(device).detach()\n",
    "        old_actions = torch.tensor(self.policy_old.actions).to(device).detach()\n",
    "        old_logprobs = torch.tensor(self.policy_old.logprobs).to(device).detach()\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            dist_entropy = self.policy(old_states, old_actions, evaluate=True)\n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            logprobs = self.policy.logprobs[0].to(device)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "            self.kl =F.kl_div(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            state_values = self.policy.state_values[0].to(device)\n",
    "            advantages = rewards - state_values.squeeze().detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            self.policy.clearMemory()\n",
    "            \n",
    "        self.policy_old.clearMemory()\n",
    "        \n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import envs.fruit_tree as ft\n",
    "env = ft.FruitTree()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 2\n",
    "action_dim = 1\n",
    "n_obj = 6\n",
    "n_episodes = 100\n",
    "max_timesteps = 500\n",
    "kl_param = 0.1\n",
    "log_interval = 10\n",
    "n_latent_var = 64           # number of variables in hidden layer\n",
    "lr = 0.0007\n",
    "betas = (0.9, 0.999)\n",
    "gamma = 0.99                # discount factor\n",
    "K_epochs = 4                # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "random_seed = None\n",
    "if random_seed:\n",
    "    torch.manual_seed(random_seed)\n",
    "    env.seed(random_seed)\n",
    "\n",
    "filename = \"PPO_FruitTree.pth\"\n",
    "directory = \"./preTrained/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "running_reward = 0\n",
    "rewards = np.zeros((n_episodes,n_obj))\n",
    "\n",
    "for ep in range(1, n_episodes+1):\n",
    "    state = np.array([0.0,0.0],dtype=np.float16)\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.policy_old(state)\n",
    "        state_n, reward, done = env.step(action)\n",
    "        # Saving state and reward:\n",
    "        ppo.policy_old.states.append(state)\n",
    "        ppo.policy_old.rewards.append(reward[0])\n",
    "        state = state_n\n",
    "        running_reward +=reward[0] \n",
    "        if done:\n",
    "            # save model\n",
    "            torch.save(ppo.policy.state_dict(), directory+filename+str(0))\n",
    "            rewards[ep-1,0]= running_reward\n",
    "            break\n",
    "        running_reward = 0\n",
    "\n",
    "    for k in range(1,n_obj):\n",
    "        state = np.array([0.0,0.0],dtype=np.float16)\n",
    "        for t in range(max_timesteps):\n",
    "            # Running policy_old:\n",
    "            action = ppo.policy_old(state)\n",
    "            state_n, reward, done = env.step(action)\n",
    "            kl = ppo.kl\n",
    "            new_reward = reward[k] + (kl_param*kl)\n",
    "\n",
    "            # Saving state and reward:\n",
    "            ppo.policy_old.states.append(state)\n",
    "            ppo.policy_old.rewards.append(new_reward)\n",
    "            state = state_n\n",
    "            running_reward += new_reward \n",
    "            if done:\n",
    "                # save model\n",
    "                torch.save(ppo.policy.state_dict(), directory+filename+str(k))\n",
    "                rewards[ep-1,k]= running_reward\n",
    "                break\n",
    "\n",
    "    print('Episode: {}\\tReward: {}'.format(ep, rewards[ep-1,:]))\n",
    "    running_reward = 0\n",
    "\n",
    "plt.plot(np.arange(len(rewards[:,0])), rewards[:,0], label = 'reward1')\n",
    "plt.plot(np.arange(len(rewards[:,1])), rewards[:,1], label = 'reward2')\n",
    "plt.plot(np.arange(len(rewards[:,0])), rewards[:,2], label = 'reward3')\n",
    "plt.plot(np.arange(len(rewards[:,1])), rewards[:,3], label = 'reward4')\n",
    "plt.plot(np.arange(len(rewards[:,0])), rewards[:,4], label = 'reward5')\n",
    "plt.plot(np.arange(len(rewards[:,1])), rewards[:,5], label = 'reward6')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.savefig('fruittree',bbox_inches='tight',facecolor=\"#FFFFFF\")\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo = PPO(state_dim, action_dim, n_latent_var, lr, betas, gamma, K_epochs, eps_clip)\n",
    "\n",
    "running_reward = 0\n",
    "rewards = []\n",
    "for ep in range(1, n_episodes+1):\n",
    "    state = np.array([0.0,0.0],dtype=np.float16)\n",
    "    for t in range(max_timesteps):\n",
    "        # Running policy_old:\n",
    "        action = ppo.policy_old(state)\n",
    "        s,r,d = env.step(action)\n",
    "        state_n = s\n",
    "        rewards = r\n",
    "        done = d\n",
    "        # Saving state and reward:\n",
    "        ppo.policy_old.states.append(state)\n",
    "        ppo.policy_old.rewards.append(reward)\n",
    "        state = state_n\n",
    "        running_reward +=reward.sum() \n",
    "        if done:\n",
    "            # save model\n",
    "            torch.save(ppo.policy.state_dict(), directory+filename)\n",
    "            rewards.append(running_reward)\n",
    "            break\n",
    "\n",
    "    print('Episode: {}\\tReward: {}'.format(ep, int(running_reward)))\n",
    "    running_reward = 0\n",
    "\n",
    "plt.plot(np.arange(len(rewards)), rewards, label = 'reward1')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.xlabel('Episode')\n",
    "plt.savefig('dst_base',bbox_inches='tight',facecolor=\"#FFFFFF\")\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}